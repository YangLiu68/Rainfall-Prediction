"""
Global sampling + correlation (Concurrent + Rate Limited + Progress)
- asyncio + aiohttp å¹¶å‘ä¸‹è½½
- ä»¤ç‰Œæ¡¶é™é€Ÿ + å…¨å±€å¹¶å‘ä¸Šé™
- tqdm è¿›åº¦æ¡ï¼šå¹¶å‘ä¸‹è½½åˆ†ç‰‡ã€é€åŸå¸‚èšåˆã€å…¨å±€åˆå¹¶ã€ç›¸å…³æ€§åˆ†æ
- æ–­ç‚¹ç»­è·‘ï¼ˆåŸå¸‚-æœˆä»½åˆ†ç‰‡è½ç›˜ï¼‰+ Ctrl+C ä¼˜é›…ä¸­æ–­

Docs: https://open-meteo.com/en/docs/historical-weather-api
"""

import os, sys, time, math, signal, warnings, asyncio, aiohttp
from datetime import date, datetime, timedelta
import numpy as np
import pandas as pd
from sklearn.feature_selection import mutual_info_regression
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

warnings.filterwarnings("ignore")

# ------------------ è·¯å¾„é…ç½® ------------------
OUT_DIR = "./dataset_global"
RAW_DIR = os.path.join(OUT_DIR, "raw_hourly")         # åˆ†ç‰‡ç¼“å­˜ï¼ˆåŸå¸‚-æœˆä»½ï¼‰
DAILY_DIR = os.path.join(OUT_DIR, "daily_by_city")    # æ¯åŸå¸‚æ—¥é¢‘
GLOBAL_DAILY_CSV = os.path.join(OUT_DIR, "weather_daily_global.csv")

# ------------------ ä»»åŠ¡é…ç½® ------------------
START_DATE = date(2015, 1, 1)
END_DATE   = date(2024, 12, 31)

# ä»£è¡¨æ€§åŸå¸‚ï¼ˆå¯æ‰©å±•ï¼‰
CITIES = [
    ("San Francisco", 37.77, -122.42), ("New York", 40.71, -74.01),
    ("Mexico City", 19.43, -99.13), ("Sao Paulo", -23.55, -46.63),
    ("Buenos Aires", -34.61, -58.38), ("Lima", -12.05, -77.05),
    ("London", 51.51, -0.13), ("Paris", 48.86, 2.35), ("Berlin", 52.52, 13.41),
    ("Moscow", 55.76, 37.62), ("Madrid", 40.42, -3.70), ("Rome", 41.90, 12.50),
    ("Cairo", 30.04, 31.24), ("Lagos", 6.46, 3.40), ("Nairobi", -1.29, 36.82),
    ("Johannesburg", -26.20, 28.04), ("Tokyo", 35.68, 139.76),
    ("Seoul", 37.57, 126.98), ("Shanghai", 31.23, 121.47),
    ("Singapore", 1.35, 103.82), ("Bangkok", 13.75, 100.50),
    ("Mumbai", 19.08, 72.88), ("Delhi", 28.61, 77.21),
    ("Dubai", 25.20, 55.27), ("Riyadh", 24.71, 46.68),
    ("Sydney", -33.87, 151.21), ("Melbourne", -37.81, 144.96),
    ("Auckland", -36.85, 174.76), ("Jakarta", -6.21, 106.85),
    ("Honolulu", 21.31, -157.86), ("Reykjavik", 64.13, -21.90),
    ("Ulaanbaatar", 47.92, 106.92),
]

# ------------------ ä¸‹è½½ä¸API ------------------
BASE_URL = "https://archive-api.open-meteo.com/v1/archive"
HOURLY_VARS = [
    "temperature_2m", "relative_humidity_2m", "surface_pressure",
    "wind_speed_10m", "wind_direction_10m", "cloud_cover", "precipitation"
]

# ------------------ å¹¶å‘ä¸é™é€Ÿ ------------------
MAX_CONCURRENCY = 8   # å¹¶å‘ä¸Šé™
MAX_RPS = 4           # æ¯ç§’è¯·æ±‚æ•°ä¸Šé™
BURST = 4             # ç¬æ—¶çªå‘ä¸Šé™
REQUEST_TIMEOUT = 40
MAX_RETRIES = 4
RETRY_BACKOFF = 1.8

# ------------------ åˆ†æé…ç½® ------------------
TARGET_COL = "precipitation_sum_mm"
SAMPLE_FRAC_GLOBAL = 0.10
SAMPLE_FRAC_PER_CITY = 0.20

# ------------------ ä¸­æ–­æ ‡è®° ------------------
stop_flag = False
def _handle_interrupt(signum, frame):
    global stop_flag
    stop_flag = True
    print("\nâš ï¸ æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼šåœæ­¢æ´¾å‘æ–°ä»»åŠ¡ï¼Œç­‰å¾…è¿›è¡Œä¸­çš„è¯·æ±‚å®Œæˆã€‚")
signal.signal(signal.SIGINT, _handle_interrupt)

# ------------------ å·¥å…·å‡½æ•° ------------------
def ensure_dirs():
    os.makedirs(OUT_DIR, exist_ok=True)
    os.makedirs(RAW_DIR, exist_ok=True)
    os.makedirs(DAILY_DIR, exist_ok=True)

def month_chunks(start_d: date, end_d: date):
    cur = date(start_d.year, start_d.month, 1)
    last = date(end_d.year, end_d.month, 1)
    while cur <= last:
        if cur.month == 12:
            month_end = date(cur.year, 12, 31)
        else:
            month_end = date(cur.year, cur.month + 1, 1) - timedelta(days=1)
        s = max(cur, start_d)
        e = min(month_end, end_d)
        yield s, e
        if cur.month == 12: cur = date(cur.year + 1, 1, 1)
        else: cur = date(cur.year, cur.month + 1, 1)

def hourly_json_to_df(payload: dict) -> pd.DataFrame:
    if "hourly" not in payload or "time" not in payload["hourly"]:
        return pd.DataFrame()
    hourly = payload["hourly"]
    df = pd.DataFrame({"time": pd.to_datetime(hourly["time"])})
    for k, v in hourly.items():
        if k == "time": continue
        df[k] = v
    df = df.set_index("time").sort_index()
    return df

def aggregate_to_daily(df_hourly: pd.DataFrame) -> pd.DataFrame:
    daily = pd.DataFrame()
    daily[TARGET_COL] = df_hourly["precipitation"].resample("D").sum(min_count=1)

    def maybe(col, how="mean"):
        if col in df_hourly.columns:
            if how == "mean": return df_hourly[col].resample("D").mean()
            if how == "max":  return df_hourly[col].resample("D").max()
            if how == "min":  return df_hourly[col].resample("D").min()
        return pd.Series(dtype=float)

    daily["temp_mean_C"]    = maybe("temperature_2m","mean")
    daily["temp_max_C"]     = maybe("temperature_2m","max")
    daily["temp_min_C"]     = maybe("temperature_2m","min")
    daily["rh_mean_pct"]    = maybe("relative_humidity_2m","mean")
    daily["press_mean_hPa"] = maybe("surface_pressure","mean")
    daily["wind_mean_ms"]   = maybe("wind_speed_10m","mean")
    daily["wind_dir_deg"]   = maybe("wind_direction_10m","mean")
    daily["cloud_mean_pct"] = maybe("cloud_cover","mean")

    # éœ²ç‚¹ï¼ˆMagnusï¼‰
    a, b = 17.62, 243.12
    T = daily["temp_mean_C"]
    RH = daily["rh_mean_pct"]
    with np.errstate(divide="ignore", invalid="ignore"):
        gamma = (a*T/(b+T)) + np.log(np.clip(RH, 1e-6, 100)/100.0)
        daily["dew_point_C"] = (b * gamma) / (a - gamma)

    # å­£èŠ‚å¾ªç¯ + æ»å/æ»šåŠ¨
    daily["month"] = daily.index.month
    daily["month_sin"] = np.sin(2*np.pi*daily["month"]/12)
    daily["month_cos"] = np.cos(2*np.pi*daily["month"]/12)
    daily["precip_lag_1"] = daily[TARGET_COL].shift(1)
    daily["precip_lag_3"] = daily[TARGET_COL].shift(3)
    daily["temp_mean_lag_1"] = daily["temp_mean_C"].shift(1)
    daily["precip_roll7"] = daily[TARGET_COL].rolling(7, min_periods=3).mean()
    return daily

# ------------------ ä»¤ç‰Œæ¡¶é™é€Ÿå™¨ ------------------
class RateLimiter:
    def __init__(self, max_rps: int, burst: int):
        self.max_rps = max(1, int(max_rps))
        self.capacity = max(1, int(burst))
        self.tokens = self.capacity
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()

    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            refill = elapsed * self.max_rps
            if refill > 0:
                self.tokens = min(self.capacity, self.tokens + refill)
                self.updated_at = now
            if self.tokens < 1:
                needed = 1 - self.tokens
                wait_s = needed / self.max_rps
                await asyncio.sleep(wait_s)
                now = time.monotonic()
                elapsed = now - self.updated_at
                refill = elapsed * self.max_rps
                self.tokens = min(self.capacity, self.tokens + refill)
                self.updated_at = now
            self.tokens -= 1

# ------------------ å¼‚æ­¥ä¸‹è½½ï¼ˆå¸¦è¿›åº¦ï¼‰ ------------------
async def fetch_hourly_chunk(session: aiohttp.ClientSession, limiter: RateLimiter,
                             lat: float, lon: float, s: date, e: date) -> dict:
    if stop_flag:
        return {}
    params = {
        "latitude": lat, "longitude": lon,
        "start_date": s.isoformat(), "end_date": e.isoformat(),
        "hourly": ",".join(HOURLY_VARS),
        "timezone": "UTC",
    }
    url = BASE_URL
    attempt = 0
    while attempt <= MAX_RETRIES and not stop_flag:
        attempt += 1
        try:
            await limiter.acquire()
            async with session.get(url, params=params, timeout=REQUEST_TIMEOUT) as resp:
                if resp.status == 200:
                    return await resp.json()
                if resp.status in (429, 500, 502, 503, 504):
                    await asyncio.sleep((RETRY_BACKOFF ** (attempt - 1)) * 0.7)
                else:
                    text = await resp.text()
                    raise RuntimeError(f"HTTP {resp.status} {text[:200]}")
        except asyncio.CancelledError:
            raise
        except Exception:
            await asyncio.sleep((RETRY_BACKOFF ** (attempt - 1)) * 0.7)
    return {}

async def concurrent_download_with_progress():
    ensure_dirs()

    # ç»Ÿè®¡æ€»åˆ†ç‰‡æ•°ï¼ˆç”¨äºæ€»è¿›åº¦ï¼‰
    all_jobs = []
    for name, lat, lon in CITIES:
        for s, e in month_chunks(START_DATE, END_DATE):
            shard = os.path.join(RAW_DIR, f"{name.replace(' ','_')}_{s}_{e}.parquet")
            all_jobs.append((name, lat, lon, s, e, shard))

    # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„åˆ†ç‰‡ï¼ˆå·²å®Œæˆå³è§†ä¸ºè¿›åº¦å·²è¾¾æˆï¼‰
    pending_jobs = [(n, lat, lon, s, e, shard) for (n, lat, lon, s, e, shard) in all_jobs if not os.path.exists(shard)]
    completed = len(all_jobs) - len(pending_jobs)

    limiter = RateLimiter(MAX_RPS, BURST)
    sem = asyncio.Semaphore(MAX_CONCURRENCY)
    timeout = aiohttp.ClientTimeout(total=None, connect=30, sock_connect=30, sock_read=REQUEST_TIMEOUT)

    pbar = tqdm(total=len(all_jobs), initial=completed, desc="å¹¶å‘ä¸‹è½½åˆ†ç‰‡", unit="shard")

    async with aiohttp.ClientSession(timeout=timeout) as session:
        async def run_job(name, lat, lon, s, e, shard):
            if stop_flag:
                return
            async with sem:
                payload = await fetch_hourly_chunk(session, limiter, lat, lon, s, e)
                if payload:
                    df = hourly_json_to_df(payload)
                    if df is not None and not df.empty:
                        df.to_parquet(shard)
                # æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½æ¨è¿›è¿›åº¦ï¼ˆå¤±è´¥çš„ä¸‹æ¬¡ä¼šé‡è¯•/è¡¥ä¸Šï¼‰
                pbar.update(1)

        tasks = [run_job(n, lat, lon, s, e, shard) for (n, lat, lon, s, e, shard) in pending_jobs]
        # é¡ºåºæ¶ˆè´¹ as_completedï¼Œä¾¿äº Ctrl+C å°½å¿«åœ
        for coro in asyncio.as_completed(tasks):
            if stop_flag:
                break
            try:
                await coro
            except Exception as ex:
                # å•åˆ†ç‰‡å¼‚å¸¸ä¸å½±å“æ•´ä½“ï¼Œè¿›åº¦å·²åœ¨ run_job ä¸­æ¨è¿›
                print(f"âš ï¸ åˆ†ç‰‡å¼‚å¸¸ï¼š{ex}")

    pbar.close()

# ------------------ èšåˆ + è¿›åº¦ ------------------
def build_city_daily_with_progress():
    files = os.listdir(RAW_DIR) if os.path.exists(RAW_DIR) else []
    city_to_shards = {}
    for fn in files:
        if not fn.endswith(".parquet"): continue
        city = fn.split("_")[0]
        city_to_shards.setdefault(city, 0)
        city_to_shards[city] += 1

    pbar = tqdm(total=len(CITIES), desc="é€åŸå¸‚èšåˆ(æ—¥é¢‘)", unit="city")
    for name, lat, lon in CITIES:
        out_path = os.path.join(DAILY_DIR, f"{name.replace(' ','_')}.parquet")
        if os.path.exists(out_path):
            pbar.update(1)
            continue
        prefix = f"{name.replace(' ','_')}_"
        shards = [os.path.join(RAW_DIR, fn) for fn in os.listdir(RAW_DIR)
                  if fn.startswith(prefix) and fn.endswith(".parquet")]
        if not shards:
            # å³ä½¿æ²¡æœ‰åˆ†ç‰‡ä¹Ÿæ¨è¿›è¿›åº¦ï¼Œé¿å…å¡ä½
            pbar.update(1)
            continue
        try:
            parts = [pd.read_parquet(p) for p in shards]
            hourly = pd.concat(parts).sort_index()
            daily = aggregate_to_daily(hourly)
            daily["city"] = name
            daily["lat"] = lat
            daily["lon"] = lon
            daily.to_parquet(out_path)
        except Exception as ex:
            print(f"âš ï¸ {name} èšåˆå¤±è´¥ï¼š{ex}")
        pbar.update(1)
    pbar.close()

def build_global_csv_with_progress():
    if os.path.exists(GLOBAL_DAILY_CSV):
        print(f"âœ… å·²æœ‰å…¨å±€ CSVï¼š{GLOBAL_DAILY_CSV}")
        return
    files = [fn for fn in os.listdir(DAILY_DIR) if fn.endswith(".parquet")]
    if not files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•åŸå¸‚æ—¥é¢‘æ•°æ®ã€‚")
        sys.exit(1)

    pbar = tqdm(total=len(files), desc="åˆå¹¶å…¨å±€CSV", unit="city")
    parts = []
    for fn in files:
        df = pd.read_parquet(os.path.join(DAILY_DIR, fn))
        parts.append(df.reset_index().rename(columns={"index":"date"}))
        pbar.update(1)
    pbar.close()

    g = pd.concat(parts, ignore_index=True)
    g.to_csv(GLOBAL_DAILY_CSV, index=False)
    print(f"ğŸ§© å…¨å±€ CSV å·²ç”Ÿæˆï¼š{GLOBAL_DAILY_CSV}")

# ------------------ ç›¸å…³æ€§ + è¿›åº¦ ------------------
def correlation_tables(df_num: pd.DataFrame, target: str):
    corr_p = df_num.corr(method="pearson")[target].sort_values(ascending=False)
    corr_s = df_num.corr(method="spearman")[target].sort_values(ascending=False)
    X = df_num.drop(columns=[target]).fillna(0)
    y = df_num[target].values
    mi = mutual_info_regression(X, y, random_state=42)
    mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)
    out = pd.DataFrame({
        "pearson": corr_p.reindex(X.columns),
        "spearman": corr_s.reindex(X.columns),
        "mutual_info": mi_series
    }).sort_values(["mutual_info","pearson"], ascending=False)
    return out

def plot_heatmap(df_num: pd.DataFrame, title: str, out_png: str, method="pearson"):
    plt.figure(figsize=(11,8))
    cmap = "coolwarm" if method == "pearson" else "BrBG"
    sns.heatmap(df_num.corr(method=method), cmap=cmap, center=0)
    plt.title(title)
    plt.tight_layout()
    plt.savefig(out_png, dpi=160)
    plt.close()

def run_correlation_with_progress():
    g = pd.read_csv(GLOBAL_DAILY_CSV, parse_dates=["time"])
    num_cols = g.select_dtypes(include=[np.number]).columns.tolist()
    if TARGET_COL not in num_cols:
        raise ValueError(f"ç›®æ ‡åˆ— {TARGET_COL} ä¸åœ¨å…¨å±€æ•°æ®ä¸­ã€‚")

    # å…¨å±€æŠ½æ ·
    g_sample = g.sample(frac=SAMPLE_FRAC_GLOBAL, random_state=42)
    g_num = g_sample[num_cols].dropna()
    out_global = correlation_tables(g_num, TARGET_COL)
    out_global.to_csv(os.path.join(OUT_DIR, "correlation_global.csv"), index=True)
    plot_heatmap(g_num, "Global Pearson Correlation", os.path.join(OUT_DIR, "global_corr_pearson.png"), "pearson")
    plot_heatmap(g_num, "Global Spearman Correlation", os.path.join(OUT_DIR, "global_corr_spearman.png"), "spearman")
    print("ğŸŒ å…¨å±€ç›¸å…³æ€§å·²è¾“å‡ºã€‚")

    # åˆ†åŸå¸‚è¿›åº¦
    pbar = tqdm(total=g["city"].nunique(), desc="åˆ†åŸå¸‚ç›¸å…³æ€§", unit="city")
    for city, dfc in g.groupby("city"):
        sample = dfc.sample(frac=min(SAMPLE_FRAC_PER_CITY, 1.0), random_state=42)
        num = sample.select_dtypes(include=[np.number]).dropna()
        if len(num) >= 20 and TARGET_COL in num.columns:
            out = correlation_tables(num, TARGET_COL)
            out.to_csv(os.path.join(OUT_DIR, f"correlation_{city.replace(' ','_')}.csv"), index=True)
        pbar.update(1)
    pbar.close()

# ------------------ ä¸»æµç¨‹ ------------------
async def _download_phase():
    await concurrent_download_with_progress()

def main():
    try:
        ensure_dirs()
        # 1) å¹¶å‘ä¸‹è½½ï¼ˆå¸¦è¿›åº¦ï¼‰
        asyncio.run(_download_phase())
        if stop_flag:
            print("â¹ï¸ å·²ä¸­æ–­ï¼šä¿ç•™å·²å®Œæˆåˆ†ç‰‡ï¼›ä¸‹æ¬¡è¿è¡Œä¼šè‡ªåŠ¨è·³è¿‡ã€‚")
            return
        # 2) èšåˆä¸ºæ—¥é¢‘ï¼ˆå¸¦è¿›åº¦ï¼‰
        build_city_daily_with_progress()
        if stop_flag:
            print("â¹ï¸ å·²ä¸­æ–­ï¼šæ—¥é¢‘èšåˆé˜¶æ®µæå‰ç»“æŸã€‚")
            return
        # 3) åˆå¹¶å…¨å±€ CSVï¼ˆå¸¦è¿›åº¦ï¼‰
        build_global_csv_with_progress()
        # 4) å…¨å±€ + åˆ†åŸå¸‚ç›¸å…³æ€§ï¼ˆå¸¦è¿›åº¦ï¼‰
        run_correlation_with_progress()
        print("âœ… å…¨æµç¨‹å®Œæˆã€‚è¾“å‡ºç›®å½•ï¼š", OUT_DIR)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œå·²å°½é‡ä¿å­˜å½“å‰æˆæœã€‚")
    except Exception as e:
        print(f"âŒ å¼‚å¸¸ï¼š{e}")
        raise

if __name__ == "__main__":
    main()
