import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt  # âœ… æ–°å¢ï¼šç”¨äºç”»å›¾

from daily_dataset import DailyWeatherDataset, DailySequenceDataset


# =========================================================
# 1. è®¾å¤‡é€‰æ‹©
# =========================================================
def get_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")  # Mac M1/M2
    else:
        device = torch.device("cpu")
    print(f"ğŸ§  ä½¿ç”¨è®¾å¤‡: {device}")
    return device


# =========================================================
# 2. æ—¥çº§ Transformer æ¨¡å‹ï¼šè¿‡å» 30 å¤© â†’ æ˜å¤©çš„ log1p(é™é›¨)
# =========================================================
class RainfallTransformer(nn.Module):
    def __init__(self, input_dim, seq_len=30, d_model=128, nhead=4, num_layers=3, dropout=0.1):
        super().__init__()
        self.input_dim = input_dim
        self.seq_len = seq_len

        self.input_proj = nn.Linear(input_dim, d_model)
        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=4 * d_model,
            dropout=dropout,
            activation="gelu",
            batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.head = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1),
        )

    def forward(self, x):
        """
        x: [batch, seq_len, input_dim]
        è¾“å‡º: [batch]ï¼Œè¡¨ç¤º log1p(mm)
        """
        if x.size(1) > self.seq_len:
            x = x[:, -self.seq_len:, :]

        x_proj = self.input_proj(x) + self.pos_embedding[:, : x.size(1)]
        enc = self.encoder(x_proj)          # [B, seq_len, d_model]
        pooled = enc.mean(dim=1)            # [B, d_model]
        out = self.head(pooled).squeeze(-1) # [B]
        return out


# =========================================================
# 3. è®­ç»ƒä¸éªŒè¯å‡½æ•°ï¼ˆDataset ç°åœ¨åªè¿”å› xb, ybï¼‰
# =========================================================
def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0

    for xb, yb in tqdm(loader, desc="ğŸ‘Ÿ è®­ç»ƒ", unit="batch"):
        xb = xb.to(device)    # [B, 30, 18]
        yb = yb.to(device)    # [B], log1p(mm)

        optimizer.zero_grad()
        pred = model(xb)      # [B]
        loss = criterion(pred, yb)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item() * xb.size(0)

    avg_loss = total_loss / len(loader.dataset)
    return avg_loss


def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0

    all_pred_log = []
    all_true_log = []

    with torch.no_grad():
        for xb, yb in tqdm(loader, desc="ğŸ” éªŒè¯", unit="batch"):
            xb = xb.to(device)
            yb = yb.to(device)

            pred = model(xb)
            loss = criterion(pred, yb)
            total_loss += loss.item() * xb.size(0)

            all_pred_log.append(pred.cpu())
            all_true_log.append(yb.cpu())

    avg_loss = total_loss / len(loader.dataset)

    # æ‹¼æˆæ•°ç»„ï¼ˆlog ç©ºé—´ï¼‰
    pred_log = torch.cat(all_pred_log).numpy()  # log1p(mm)
    true_log = torch.cat(all_true_log).numpy()

    # log ç©ºé—´ RMSE
    rmse_log = float(np.sqrt(np.mean((pred_log - true_log) ** 2)))

    # è¿˜åŸåˆ° mm ç©ºé—´
    pred_mm = np.expm1(pred_log)
    true_mm = np.expm1(true_log)

    rmse_mm = float(np.sqrt(np.mean((pred_mm - true_mm) ** 2)))
    mae_mm = float(np.mean(np.abs(pred_mm - true_mm)))

    return avg_loss, rmse_log, rmse_mm, mae_mm


# =========================================================
# 4. mainï¼šå‡†å¤‡æ•°æ®ã€æ¨¡å‹ã€è®­ç»ƒå¾ªç¯
# =========================================================
def main():
    device = get_device()

    # ---- 1) æ„å»º Dataset ----
    daily_ds = DailyWeatherDataset(
        csv_path="./dataset_global/weather_daily_global.csv",
        cities=("San Francisco", "New York"),
        date_col="time",
    )
    seq_ds = DailySequenceDataset(daily_ds, lookback=30)

    # ---- 2) åˆ’åˆ†è®­ç»ƒ / éªŒè¯ ----
    val_ratio = 0.2
    val_size = int(len(seq_ds) * val_ratio)
    train_size = len(seq_ds) - val_size

    train_ds, val_ds = random_split(
        seq_ds,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42),
    )

    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)

    # ---- 3) åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨ ----
    input_dim = len(daily_ds.feature_cols)  # 18
    seq_len = 30

    model = RainfallTransformer(input_dim=input_dim, seq_len=seq_len).to(device)

    criterion = nn.HuberLoss(delta=0.1)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)

    best_val_loss = float("inf")
    best_path = "daily_transformer_best.pt"

    # âœ… æ–°å¢ï¼šç”¨æ¥è®°å½•æ¯ä¸ª epoch çš„ loss
    train_losses = []
    val_losses = []

    # ---- 4) è®­ç»ƒè‹¥å¹² epoch ----
    num_epochs = 15

    for epoch in range(1, num_epochs + 1):
        print(f"\n===== ğŸŒ€ Epoch {epoch}/{num_epochs} =====")

        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)
        print(f"ğŸ“‰ è®­ç»ƒé›†å¹³å‡æŸå¤±: {train_loss:.6f}")

        val_loss, rmse_log, rmse_mm, mae_mm = evaluate(model, val_loader, criterion, device)
        print(f"âœ… éªŒè¯é›†å¹³å‡æŸå¤±: {val_loss:.6f}")
        print(f"   â†³ RMSE(logç©ºé—´): {rmse_log:.4f}")
        print(f"   â†³ RMSE(mm):     {rmse_mm:.4f}")
        print(f"   â†³ MAE(mm):      {mae_mm:.4f}")

        # âœ… è®°å½• loss
        train_losses.append(train_loss)
        val_losses.append(val_loss)

        # ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), best_path)
            print(f"ğŸ’¾ å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼Œå·²ä¿å­˜åˆ°: {best_path}")

    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"â­ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"â­ æœ€ä¼˜æ¨¡å‹æƒé‡ä¿å­˜åœ¨: {best_path}")

    # âœ… è®­ç»ƒç»“æŸåç”» loss æ›²çº¿
    epochs = range(1, num_epochs + 1)
    plt.figure(figsize=(8, 5))
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.plot(epochs, val_losses, label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Daily Transformer Training & Validation Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("loss_curve.png", dpi=150)
    print("ğŸ“ˆ å·²ä¿å­˜ loss æ›²çº¿åˆ° loss_curve.png")
    # å¦‚æœä½ åœ¨æœ¬åœ°è·‘ï¼Œå¹¶ä¸”æƒ³å¼¹å‡ºçª—å£çœ‹å›¾ï¼Œå¯ä»¥å–æ¶ˆä¸‹ä¸€è¡Œæ³¨é‡Š
    # plt.show()


if __name__ == "__main__":
    main()
